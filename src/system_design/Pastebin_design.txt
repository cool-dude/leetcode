Use cases:
	• User enters block of text and gets randomly generated link.
		○ Expiration
			§ Default setting does not expire
			§ Can optionally set timed expiration
	• User enters paste's url and view contents.
	• User anonymous.
	• Service tracks analytics pages.
		○ Monthly visit stats.
	• Service deletes expired pastes.
	• Service has high availability.
Out of scope
	• User registers for account.
		○ User verifies email.
	• User logs into registered account.
		○ User edits the document.
	• User can set visibility.
	• User can set shortlink.
State assumptions
	• Traffic not evenly distributed
	• Following short link  fast
	• Pastes are text only
	• Page view analytics not real-time.
	• 10M users.
	• 10M writes/month.
	• 100M reads/month.
	• 10:1 read write ratio.
Calculate usage:
	• Size per paste
		○ 1 KB content per paste
		○ shortlink - 7 bytes
		○ expiration_length_in_minutes - 4 bytes
		○ created_at - 5 bytes
		○ paste_path - 255 bytes
		○ total = ~1.27 KB
	• 12.7 GB of new paste content per month
		○ 1.27 KB per paste * 10 million pastes per month
		○ ~450 GB of new paste content in 3 years
		○ 360 million shortlinks 3 years
		○ Assume most new pastes instead of updates to existing ones
	• 4 paste writes/second
	• 40 reads/second
Handy conversion guide:
	• 2.5M seconds/month
	• 1 requests/second = 2.5M requests/month
	• 40 requests/second = 100M requests/month
	• 400 requests/second = 1B requests/month

	The pastes table could have the following structure:
	shortlink char(7) NOT NULL
    expiration_length_in_minutes int NOT NULL
    created_at datetime NOT NULL
    paste_path varchar(255) NOT NULL
    PRIMARY KEY(shortlink)
	We'll create an index on shortlink and created_at to speed up lookups (log-time instead of scanning the entire table) and to keep the data in memory. Reading 1 MB sequentially from memory takes about 250 microseconds, while reading from SSD takes 4x and from disk takes 80x longer.1
	To generate unique url, we could:
		• MD5 hash of the user's ip_address + timestamp
			○ MD5 is a widely used hashing function that produces a 128-bit hash value
			○ MD5 is uniformly distributed
			○ Alternatively, we could also take the MD5 hash of randomly-generated data
		• Base 62 encode MD5 hash
			○ Base 62 encodes to [a-zA-Z0-9] which works well for urls, eliminating the need for escaping special characters
			○ There is only one hash result for the original input and Base 62 is deterministic (no randomness involved)
			○ Base 64 is another popular encoding but provides issues for urls because of the additional + and / characters
			○ The following Base 62 pseudocode runs in O(k) time where k is the number of digits = 7:

	def base_encode(num, base=62):
    digits = []
    while num > 0
      remainder = modulo(num, base)
      digits.push(remainder)
      num = divide(num, base)
    digits = digits.reverse

		• Take the first 7 characters of the output, which results in 62^7 possible values and should be sufficient to handle our constraint of 360 million shortlinks in 3 years:
	url = base_encode(md5(ip_address+timestamp))[:URL_LENGTH]
	We'll use a public REST API:
	$ curl -X POST --data '{ "expiration_length_in_minutes": "60", \
    "paste_contents": "Hello World!" }' https://pastebin.com/api/v1/paste
	Response:{
        "shortlink": "foobar"
    }
	For internal communications, we could use Remote Procedure Calls.

	Use case: User enters a paste's url and views the contents
		• The Client sends a get paste request to the Web Server
		• The Web Server forwards the request to the Read API server
		• The Read API server does the following:
			○ Checks the SQL Database for the generated url
				§ If the url is in the SQL Database, fetch the paste contents from the Object Store
				§ Else, return an error message for the user
	REST API:
	$ curl https://pastebin.com/api/v1/paste?shortlink=foobar
	Response:{
        "paste_contents": "Hello World"
        "created_at": "YYYY-MM-DD HH:MM:SS"
        "expiration_length_in_minutes": "60"
    }

    Use case: Service tracks analytics of pages
    Since realtime analytics are not requirement,
    we could simply MapReduce the Web Server logs to
    generate hit counts.class HitCounts(MRJob):
    def extract_url(self, line):
        """Extract the generated url from the log line."""
        ...
    def extract_year_month(self, line):
        """Return the year and month portions of the timestamp."""
        ...
    def mapper(self, _, line):
        """Parse each log line, extract and transform relevant lines.
    Emit key value pairs of the form:
    (2016-01, url0), 1
        (2016-01, url0), 1
        (2016-01, url1), 1
        """
        url = self.extract_url(line)
        period = self.extract_year_month(line)
        yield (period, url), 1
    def reducer(self, key, values):
        """Sum values for each key.
    (2016-01, url0), 2
        (2016-01, url1), 1
        """
        yield key, sum(values)
    Use case: Service deletes expired pastes
    To delete expired pastes, we could just scan the SQL Database for all
    entries whose expirationx timestamp are older than the current timestamp.
    All expired entries would then be deleted (or marked as expired) from
    the table.

    Scaled: