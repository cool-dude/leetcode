We'll scope the problem to handle only the following use case
	• Service calculates the past week's most popular products by category
	• User views the past week's most popular products by category
	• Service has high availability
Out of scope
	The general e-commerce site
		Design components only for calculating sales rank.
State assumptions
	• Traffic is not evenly distributed
	• Items can be in multiple categories
	• Items cannot change categories
	• There are no subcategories ie foo/bar/baz
	• Results must be updated hourly
		○ More popular products might need to be updated more frequently
	• 10M products
	• 1000 categories
	• 1B transactions/month
	• 100B read requests/month
	• 100:1 read to write ratio
Calculate usage:
Size per transaction:
		○ created_at - 5 bytes
		○ product_id - 8 bytes
		○ category_id - 4 bytes
		○ seller_id - 8 bytes
		○ buyer_id - 8 bytes
		○ quantity - 4 bytes
		○ total_price - 5 bytes
		○ Total: ~40 bytes
	• 40 GB of new content /month
		○ 40 bytes/transaction * 1B transactions/month.
		○ 1.44 TB of new transaction content in 3 years.
		○ Assume most are new transactions instead of updates to existing ones
	• 400 transactions/sec on average.
	• 40,000 read requests/second on average.
Handy conversion guide:
	• 2.5M seconds/month
	• 1 request/second = 2.5M requests/month
	• 40 requests/second = 100M requests/month
	• 400 requests/second = 1B requests/month.
Use case: Service calculates the past
week's most popular products by category

We could store the raw Sales API server
log files on a managed Object Store Amazon S3,
rather than managing our own distributed file system.
We'll assume this is a sample log entry,
tab delimited:
timestamp   product_id  category_id    qty     total_price   seller_id    buyer_id
t1          product1    category1      2       20.00         1            1
t2          product1    category2      2       20.00         2            2
t2          product1    category2      1       10.00         2            3
t3          product2    category1      3        7.00         3            4
t4          product3    category2      7        2.00         4            5
t5          product4    category1      1        5.00         5            6
The Sales Rank Service could use MapReduce,
using the Sales API server log files as input
and writing the results to an aggregate table sales_rank in a SQL Database.
We should discuss the use cases and tradeoffs between choosing SQL or NoSQL.
	We'll use a multi-step MapReduce:
		• Step 1 - Transform the data to (category, product_id), sum(quantity).
		• Step 2 - Perform a distributed sort.
	The result would be the following sorted list, which we could insert into the sales_rank table:
(category1, 1), product4
(category1, 2), product1
(category1, 3), product2
(category2, 3), product1
(category2, 7), product3

class SalesRanker(MRJob):
	def within_past_week(self, timestamp):
        """Return True if timestamp is within past week, False otherwise."""
        ...
    def mapper(self, _ line):
        """Parse each log line, extract and transform relevant lines.
        Emit key value pairs of the form:
        (category1, product1), 2
        (category2, product1), 2
        (category2, product1), 1
        (category1, product2), 3
        (category2, product3), 7
        (category1, product4), 1
        """
        timestamp, product_id, category_id, quantity, total_price, seller_id, \
            buyer_id = line.split('\t')
        if self.within_past_week(timestamp):
            yield (category_id, product_id), quantity
    def reducer(self, key, value):
        """Sum values for each key.
        (category1, product1), 2
        (category2, product1), 3
        (category1, product2), 3
        (category2, product3), 7
        (category1, product4), 1
        """
        yield key, sum(values)
    def mapper_sort(self,key,value):
        "Construct key for proper sorting"
        Transform key and value to the form:
        (category1, 2), product1
        (category2, 3), product1
        (category1, 3), product2
        (category2, 7), product3
        (category1, 1), product4
        The shuffle/sort step of MapReduce will then do a
        distributed sort on the keys, resulting in:
        (category1, 1), product4
        (category1, 2), product1
        (category1, 3), product2
        """
        category_id, product_id = key
        quantity = value
        yield (category_id, quantity), product_id
	def reducer_identity(self, key, value):
        yield key, value
	def steps(self):
        """Run the map and reduce steps."""
        return [
            self.mr(mapper=self.mapper,
                    reducer=self.reducer),
            self.mr(mapper=self.mapper_sort,
                    reducer=self.reducer_identity),
        ]
The sales_rank table could have the following structure:
	id int NOT NULL AUTO_INCREMENT
    category_id int NOT NULL
    total_sold int NOT NULL
    product_id int NOT NULL
    PRIMARY KEY(id)
    FOREIGN KEY(category_id) REFERENCES Categories(id)
    FOREIGN KEY(product_id) REFERENCES Products(id)

Use case: User views the past week's most popular products by category
	• The Client sends a request to the Web Server, running as a reverse proxy
	• The Web Server forwards the request to the Read API server
	• The Read API server reads from the SQL Database sales_rank table
We'll use a public REST API:
$ curl https://amazon.com/api/v1/popular?category_id=1234
Response:{
    "id": "100",
    "category_id": "1234",
    "total_sold": "100000",
    "product_id": "50",
},
{
    "id": "53",
    "category_id": "1234",
    "total_sold": "90000",
    "product_id": "200",
},
{
    "id": "75",
    "category_id": "1234",
    "total_sold": "80000",
    "product_id": "3",
},
For internal communications, we could use Remote Procedure Calls.