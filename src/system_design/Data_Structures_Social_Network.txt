Step 1: Outline use cases and constraints
Gather requirements and scope the problem.
Ask questions to clarify use cases and
constraints. Discuss assumptions.

Without an interviewer to address clarifying questions,
we'll define some use cases and constraints.

Use cases
We'll scope the problem to handle only the following use cases
User searches for someone and sees the shortest path to the searched person
Service has high availability
Constraints and assumptions
State assumptions
Traffic is not evenly distributed
Some searches are more popular than others,
while others are only executed once
Graph data won't fit on a single machine
Graph edges are unweighted
100 million users
50 friends per user average
1 billion friend searches per month
Exercise the use of more traditional systems -
don't use graph-specific solutions such as GraphQL or a graph database like Neo4j

Calculate usage
Clarify with your interviewer if you should
run back-of-the-envelope usage calculations.

5 billion friend relationships
100 million users * 50 friends per user average
400 search requests per second
Handy conversion guide:
2.5 million seconds per month
1 request per second = 2.5 million requests per month
40 requests per second = 100 million requests per month
400 requests per second = 1 billion requests per month
Step 2: Create a high level design
Outline a high level design with all important components.

Step 3: Design core components
Dive into details for each core component.

Use case: User searches for someone and sees
the shortest path to the searched person
Clarify with your interviewer how much code
you are expected to write.

Without the constraint of millions of users (vertices)
and billions of friend relationships (edges),
we could solve this unweighted shortest path task with a general BFS approach:

class Graph(Graph):
    def shortest_path(self, src, dst):
        if src is None or dst is None:
            return None
        if src is dst:
            return [src.key]
        prev_node_keys = self._shortest_path(src, dst)
        if prev_node_keys is None:
            return None
        else:
            path_ids = [dst.key]
            prev_node_key = prev_node_keys[dst.key]
            while prev_node_key is not None:
                path_ids.append(prev_node_key)
                prev_node_key = prev_node_keys[prev_node_key]
            return path_ids[::-1]
    def _shortest_path(self, src, dst):
        queue = deque()
        queue.append(source)
        prev_node_keys = {source.key: None}
        source.visit_state = State.visited
        while queue:
            node = queue.popleft()
            if node is dst:
                return prev_node_keys
            prev_node = node
            for adj_node in node.adj_nodes.values():
                if adj_node.visit_state == State.unvisited:
                    queue.append(adj_node)
                    prev_node_keys[adj_node.key] = prev_node.key
                    adj_node.visit_state = State.visited
        return None
We won't be able to fit all users on the same machine,
we'll need to shard users across Person Servers
and access them with a Lookup Service.

The Client sends a request to the Web Server,
running as a reverse proxy
The Web Server forwards the request to the Search API server
The Search API server forwards the request to
the User Graph Service
The User Graph Service does the following:
Uses the Lookup Service to find the Person Server
where the current user's info is stored
Finds the appropriate Person Server to retrieve
the current user's list of friend_ids
Runs a BFS search using the current user as the
source and the current user's friend_ids as the
ids for each adjacent_node
To get the adjacent_node from a given id:
The User Graph Service will again need to communicate
with the Lookup Service to determine which Person Server
stores the adjacent_node matching the given id (potential for optimization)
Clarify with your interviewer how much code you should be writing.

Note: Error handling is excluded below for simplicity.
Ask if you should code proper error handing.

Lookup Service implementation:
class LookupService(object):
    def __init__(self):
        self.lookup = self._init_lookup()  # key: person_id, value: person_server
    def _init_lookup(self):
        ...
    def lookup_person_server(self, person_id):
        return self.lookup[person_id]

Person Server implementation:
class PersonServer(object):
    def __init__(self):
        self.people = {}  # key: person_id, value: person
    def add_person(self, person):
        ...
    def people(self, ids):
        results = []
        for id in ids:
            if id in self.people:
                results.append(self.people[id])
        return results

Person implementation:
class Person(object):
    def __init__(self, id, name, friend_ids):
        self.id = id
        self.name = name
        self.friend_ids = friend_ids

User Graph Service implementation:
class UserGraphService(object):
    def __init__(self, lookup_service):
        self.lookup_service = lookup_service
    def person(self, person_id):
        person_server = self.lookup_service.lookup_person_server(person_id)
        return person_server.people([person_id])
    def shortest_path(self, source_key, dest_key):
        if source_key is None or dest_key is None:
            return None
        if source_key is dest_key:
            return [source_key]
        prev_node_keys = self._shortest_path(source_key, dest_key)
        if prev_node_keys is None:
            return None
        else:
            # Iterate through the path_ids backwards, starting at dest_key
            path_ids = [dest_key]
            prev_node_key = prev_node_keys[dest_key]
            while prev_node_key is not None:
                path_ids.append(prev_node_key)
                prev_node_key = prev_node_keys[prev_node_key]
            # Reverse the list since we iterated backwards
            return path_ids[::-1]

    def _shortest_path(self, source_key, dest_key, path):
        # Use the id to get the Person
        source = self.person(source_key)
        # Update our bfs queue
        queue = deque()
        queue.append(source)
        # prev_node_keys keeps track of each hop from
        # the source_key to the dest_key
        prev_node_keys = {source_key: None}
        # We'll use visited_ids to keep track of which nodes we've
        # visited, which can be different from a typical bfs where
        # this can be stored in the node itself
        visited_ids = set()
        visited_ids.add(source.id)
        while queue:
            node = queue.popleft()
            if node.key is dest_key:
                return prev_node_keys
            prev_node = node
            for friend_id in node.friend_ids:
                if friend_id not in visited_ids:
                    friend_node = self.person(friend_id)
                    queue.append(friend_node)
                    prev_node_keys[friend_id] = prev_node.key
                    visited_ids.add(friend_id)
        return None
We'll use a public REST API:
$ curl https://social.com/api/v1/friend_search?person_id=1234
Response:
{
    "person_id": "100",
    "name": "foo",
    "link": "https://social.com/foo",
},
{
    "person_id": "53",
    "name": "bar",
    "link": "https://social.com/bar",
},
{
    "person_id": "1234",
    "name": "baz",
    "link": "https://social.com/baz",
},
For internal communications, we could use Remote Procedure Calls.

Expanding the Memory Cache to many machines
To handle the heavy request load and the large
amount of memory needed, we'll scale horizontally.
We have three main options on how to store
the data on our Memory Cache cluster:

Each machine in the cache cluster has its own cache -
Simple, although it will likely result in a low
cache hit rate.
Each machine in the cache cluster has a copy of the
cache - Simple, although it is an inefficient
use of memory.
The cache is sharded across all machines in the cache
cluster - More complex, although it is likely the best option.
We could use hashing to determine which machine could
have the cached results of a query using machine = hash(query).
We'll likely want to use consistent hashing.